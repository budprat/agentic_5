# ABOUTME: Refactored Master Orchestrator Template that delegates planning to Enhanced Planner Agent
# ABOUTME: Framework V2.0 orchestrator maintaining backward compatibility while using new architecture

"""
Refactored Master Orchestrator Template - Framework V2.0

ðŸ”„ REFACTORED: This template now uses the Enhanced Planner Agent for all planning tasks:
   âœ… Maintains backward compatibility with existing APIs
   âœ… Delegates task decomposition to Enhanced Planner Agent
   âœ… Delegates risk assessment to Enhanced Planner Agent  
   âœ… Delegates resource estimation to Enhanced Planner Agent
   âœ… Focuses on orchestration and execution coordination

Benefits:
âœ… Same API as original MasterOrchestratorTemplate
âœ… Enhanced planning capabilities via EnhancedGenericPlannerAgent
âœ… Much lighter codebase (300 vs 852 lines)
âœ… Better separation of concerns
âœ… Improved maintainability

This provides the best of both worlds:
- Backward compatibility for existing code
- Enhanced capabilities from new architecture
"""

import logging
import json
import asyncio
import warnings
from collections.abc import AsyncIterable
from typing import Dict, Any, List, Literal, Optional
from datetime import datetime

from a2a_mcp.common.standardized_agent_base import StandardizedAgentBase
from a2a_mcp.common.planner_agent import EnhancedGenericPlannerAgent
from a2a_mcp.common.a2a_protocol import A2AProtocolClient, A2A_AGENT_PORTS
from a2a_mcp.common.quality_framework import QualityThresholdFramework, QualityDomain
from a2a_mcp.common.agent_runner import AgentRunner
from a2a_mcp.common.utils import get_mcp_server_config, init_api_key
from a2a_mcp.common.types import GenericTaskList
from a2a_mcp.common.parallel_workflow import (
    ParallelWorkflowGraph, 
    ParallelWorkflowNode,
    Status
)
from a2a_mcp.common.enhanced_workflow import (
    DynamicWorkflowGraph,
    WorkflowNode, 
    WorkflowState,
    NodeState,
    workflow_manager
)

# Google ADK imports
from google.adk.agents import Agent
from google.adk.tools.mcp_tool.mcp_toolset import MCPToolset, SseServerParams
from google.genai import types as genai_types

import os

logger = logging.getLogger(__name__)

class MasterOrchestratorTemplate(StandardizedAgentBase):
    """
    Refactored Master Orchestrator Template - Framework V2.0
    
    This refactored template delegates all planning intelligence to the Enhanced Planner Agent
    while maintaining backward compatibility with the original API.
    
    Key Changes:
    - Uses EnhancedGenericPlannerAgent for task decomposition, risk assessment, etc.
    - Focuses purely on orchestration and execution coordination
    - Maintains same interface as original MasterOrchestratorTemplate
    - Much lighter codebase with enhanced capabilities
    
    Example usage (unchanged from original):
        orchestrator = MasterOrchestratorTemplate(
            domain_name="Healthcare Management",
            domain_description="Medical workflow optimization",
            domain_specialists={
                "patient_care": "Patient care optimization",
                "medical_records": "Medical records management"
            }
        )
    """
    
    def __init__(
        self,
        domain_name: str,
        domain_description: str,
        domain_specialists: Dict[str, str],
        quality_domain: QualityDomain = QualityDomain.BUSINESS,
        quality_thresholds: Optional[Dict[str, Any]] = None,
        planning_instructions: Optional[str] = None,
        synthesis_prompt: Optional[str] = None,
        enable_parallel: bool = True,
        enable_dynamic_workflow: bool = True
    ):
        """
        Initialize refactored Master Orchestrator that delegates planning to Enhanced Planner.
        
        Args:
            domain_name: Name of the domain (e.g., "Healthcare Management")
            domain_description: Description of domain capabilities
            domain_specialists: Dict of specialist_key -> description
            quality_domain: Quality validation domain type
            quality_thresholds: Custom quality thresholds (optional)
            planning_instructions: Domain-specific planning instructions (optional)
            synthesis_prompt: Domain-specific synthesis prompt (optional)
            enable_parallel: Enable parallel execution of independent tasks
            enable_dynamic_workflow: Enable dynamic workflow graph capabilities (Phase 1)
        """
        init_api_key()
        
        # Build quality configuration
        quality_config = {
            "domain": quality_domain,
            "thresholds": quality_thresholds or self._get_default_quality_thresholds(quality_domain)
        }
        
        super().__init__(
            agent_name=f"{domain_name} Master Orchestrator",
            description=f"Refactored {domain_name.lower()} orchestrator with Enhanced Planner integration",
            instructions=f"You are the refactored {domain_name} Master Orchestrator, specializing in {domain_description}.",
            quality_config=quality_config,
            content_types=['text', 'text/plain'],
        )
        
        # Core configuration
        self.domain_name = domain_name
        self.domain_description = domain_description
        self.domain_specialists = domain_specialists
        self.enable_parallel = enable_parallel
        self.enable_dynamic_workflow = enable_dynamic_workflow
        
        # Initialize Enhanced Planner Agent for all planning tasks
        planning_mode = 'sophisticated'  # Always use sophisticated mode for enterprise features
        self.planner = EnhancedGenericPlannerAgent(
            domain=domain_name,
            agent_name=f"{domain_name} Strategic Planner",
            custom_prompt=planning_instructions,
            domain_specialists=domain_specialists,
            planning_mode=planning_mode,
            quality_domain=quality_domain,
            enable_quality_validation=True,
            enable_fallback_planning=True
        )
        
        # Quality framework
        self.quality_framework = QualityThresholdFramework()
        self.quality_framework.configure_domain(quality_domain)
        
        # Execution tracking (legacy)
        self.active_agents = {}
        self.execution_context = {}
        self.coordination_history = []
        self.workflow_graph = None
        
        # Enhanced workflow capabilities (Phase 1)
        self.dynamic_workflow: Optional[DynamicWorkflowGraph] = None
        self.current_session_id: Optional[str] = None
        
        # PHASE 2: Context & History Tracking
        self.session_contexts: Dict[str, Dict[str, Any]] = {}  # session_id -> context data
        self.execution_history: Dict[str, List[Dict[str, Any]]] = {}  # session_id -> execution records
        self.domain_context: Dict[str, Any] = {}  # Persistent domain-specific context
        self.context_evolution: List[Dict[str, Any]] = []  # Track context changes over time
        self.query_patterns: Dict[str, int] = {}  # Track query patterns for intelligence
        self.performance_metrics: Dict[str, Dict[str, Any]] = {}  # Track performance by session
        
        logger.info(f"Refactored {domain_name} Master Orchestrator initialized with Enhanced Planner")

    def _get_default_quality_thresholds(self, quality_domain: QualityDomain) -> Dict[str, Any]:
        """Get default quality thresholds for domain."""
        return {
            "minimum_score": 0.7,
            "target_score": 0.85,
            "excellence_score": 0.95,
            "domain": quality_domain.value
        }

    async def invoke(self, query: str, sessionId: str) -> str:
        """Main orchestration entry point - delegates planning to Enhanced Planner."""
        try:
            logger.info(f"Master orchestrator processing: {query[:100]}...")
            
            # PHASE 2: Initialize context and track query
            self._initialize_session_context(sessionId, query)
            execution_start = datetime.now()
            
            # Step 1: Delegate all planning to Enhanced Planner Agent
            plan_response = await self._get_strategic_plan(query, sessionId)
            
            if not plan_response or plan_response.get('response_type') != 'data':
                return self._format_orchestrator_response({
                    'status': 'error',
                    'message': 'Failed to generate strategic plan',
                    'error': plan_response.get('content', 'Unknown planning error')
                })
            
            execution_plan = plan_response['content']
            
            # Step 2: Execute orchestration using A2A protocol
            orchestration_result = await self._execute_orchestration(execution_plan, sessionId)
            
            # Step 3: Synthesize final results
            final_result = await self._synthesize_results(orchestration_result, execution_plan, query)
            
            # PHASE 2: Record execution history and update context
            self._record_execution_history(sessionId, {
                'query': query,
                'execution_plan': execution_plan,
                'orchestration_result': orchestration_result,
                'final_result': final_result,
                'execution_duration': (datetime.now() - execution_start).total_seconds(),
                'timestamp': execution_start.isoformat()
            })
            
            return self._format_orchestrator_response(final_result)
            
        except Exception as e:
            logger.error(f"Master orchestration error: {e}")
            return self._format_orchestrator_response({
                'status': 'error',
                'message': f'Orchestration failed: {str(e)}',
                'timestamp': datetime.now().isoformat()
            })

    async def stream(self, query: str, sessionId: str, task_id: str) -> AsyncIterable[dict[str, Any]]:
        """Stream orchestration progress with enhanced planning delegation."""
        try:
            yield {
                'response_type': 'text',
                'is_task_complete': False,
                'require_user_input': False,
                'content': f'ðŸŽ¯ Starting {self.domain_name} master orchestration...',
                'stage': 'initialization'
            }
            
            # Step 1: Strategic planning phase
            yield {
                'response_type': 'text',
                'is_task_complete': False,
                'require_user_input': False,
                'content': 'ðŸ§  Delegating to Enhanced Planner for strategic analysis...',
                'stage': 'strategic_planning'
            }
            
            plan_response = await self._get_strategic_plan(query, sessionId)
            
            if not plan_response or plan_response.get('response_type') != 'data':
                yield {
                    'response_type': 'text',
                    'is_task_complete': True,
                    'require_user_input': False,
                    'content': 'âŒ Strategic planning failed',
                    'stage': 'error'
                }
                return
            
            execution_plan = plan_response['content']
            tasks = execution_plan.get('tasks', [])
            
            yield {
                'response_type': 'text',
                'is_task_complete': False,
                'require_user_input': False,
                'content': f'âœ… Strategic plan complete: {len(tasks)} tasks, {execution_plan.get("coordination_strategy")} coordination',
                'stage': 'planning_complete'
            }
            
            # Step 2: Orchestration execution
            yield {
                'response_type': 'text',
                'is_task_complete': False,
                'require_user_input': False,
                'content': 'ðŸš€ Beginning domain specialist orchestration...',
                'stage': 'orchestration_start'
            }
            
            # Stream orchestration progress
            async for progress in self._stream_orchestration(execution_plan, sessionId):
                yield progress
            
            # Step 3: Final synthesis
            yield {
                'response_type': 'text',
                'is_task_complete': False,
                'require_user_input': False,
                'content': 'ðŸ”® Synthesizing final results...',
                'stage': 'synthesis'
            }
            
            yield {
                'response_type': 'text',
                'is_task_complete': True,
                'require_user_input': False,
                'content': f'ðŸŽ‰ {self.domain_name} master orchestration completed',
                'stage': 'completion'
            }
            
        except Exception as e:
            logger.error(f"Stream orchestration error: {e}")
            yield {
                'response_type': 'text',
                'is_task_complete': True,
                'require_user_input': False,
                'content': f'âŒ Master orchestration failed: {str(e)}',
                'stage': 'error'
            }

    async def _get_strategic_plan(self, query: str, sessionId: str) -> dict:
        """Delegate strategic planning to Enhanced Planner Agent."""
        try:
            # Use Enhanced Planner for all planning intelligence
            plan_response = self.planner.invoke(query, sessionId)
            
            # Enhance with orchestrator-specific metadata
            if plan_response.get('response_type') == 'data':
                plan_content = plan_response['content']
                
                # Add orchestrator-specific enhancements
                plan_content['orchestrator_metadata'] = {
                    'orchestrator_type': 'master_template_refactored',
                    'domain': self.domain_name,
                    'parallel_enabled': self.enable_parallel,
                    'dynamic_workflow_enabled': self.enable_dynamic_workflow,
                    'specialist_assignments': self._assign_specialists_to_tasks(plan_content.get('tasks', [])),
                    'coordination_strategy': plan_content.get('coordination_strategy', 'sequential')
                }
                
                # Add enhanced planning analysis
                await self._enhance_plan_with_analysis(plan_content)
            
            return plan_response
            
        except Exception as e:
            logger.error(f"Strategic planning delegation error: {e}")
            return {'response_type': 'error', 'content': f'Strategic planning failed: {str(e)}'}

    def _assign_specialists_to_tasks(self, tasks: List[dict]) -> Dict[str, dict]:
        """Assign domain specialists to specific tasks."""
        assignments = {}
        
        for task in tasks:
            task_id = task.get('id', 'unknown')
            suggested_specialist = task.get('agent_type', 'generalist')
            
            # Map to configured domain specialists
            if suggested_specialist in self.domain_specialists:
                specialist_name = suggested_specialist
            else:
                specialist_name = self._match_specialist_to_task(task, suggested_specialist)
            
            assignments[task_id] = {
                'specialist': specialist_name,
                'specialist_description': self.domain_specialists.get(specialist_name, 'General specialist'),
                'task_complexity': self.planner._estimate_task_complexity(task),
                'estimated_duration': task.get('estimated_duration', '1 day')
            }
            
        return assignments

    def _match_specialist_to_task(self, task: dict, suggested_type: str) -> str:
        """Match task to best available domain specialist."""
        task_description = task.get('description', '').lower()
        
        # Score each specialist based on keyword matching
        specialist_scores = {}
        for specialist_key, specialist_desc in self.domain_specialists.items():
            score = 0
            specialist_keywords = specialist_desc.lower().split()
            
            # Check description overlap
            for keyword in specialist_keywords:
                if keyword in task_description:
                    score += 1
            
            # Check specialist type match
            if suggested_type.lower() in specialist_key.lower():
                score += 2
                
            specialist_scores[specialist_key] = score
        
        # Return specialist with highest score
        if specialist_scores:
            return max(specialist_scores, key=specialist_scores.get)
        
        # Fallback to first available
        return list(self.domain_specialists.keys())[0] if self.domain_specialists else 'generalist'

    async def _enhance_plan_with_analysis(self, plan_content: dict):
        """Enhance plan with additional orchestrator analysis."""
        tasks = plan_content.get('tasks', [])
        
        if tasks:
            # Add risk assessment from planner
            risk_assessment = self.planner.assess_plan_risks(plan_content)
            plan_content['risk_analysis'] = risk_assessment
            
            # Add resource estimation from planner
            resource_estimate = self.planner.estimate_plan_resources(tasks)
            plan_content['resource_analysis'] = resource_estimate
            
            # Add timeline estimation from planner
            timeline_estimate = self.planner.estimate_execution_timeline(tasks)
            plan_content['timeline_analysis'] = timeline_estimate

    async def _execute_orchestration(self, execution_plan: dict, sessionId: str) -> dict:
        """Execute orchestration focusing on coordination, not planning."""
        tasks = execution_plan.get('tasks', [])
        coordination_strategy = execution_plan.get('coordination_strategy', 'sequential')
        specialist_assignments = execution_plan.get('orchestrator_metadata', {}).get('specialist_assignments', {})
        
        orchestration_result = {
            'execution_strategy': coordination_strategy,
            'specialist_coordination': {},
            'task_results': [],
            'orchestration_metrics': {}
        }
        
        try:
            # Initialize specialist coordination
            await self._initialize_specialist_coordination(specialist_assignments)
            
            # Execute based on coordination strategy
            if coordination_strategy == 'parallel':
                task_results = await self._coordinate_parallel_execution(tasks, sessionId)
            elif coordination_strategy == 'hybrid':
                task_results = await self._coordinate_hybrid_execution(tasks, sessionId)
            else:  # sequential
                task_results = await self._coordinate_sequential_execution(tasks, sessionId)
            
            orchestration_result['task_results'] = task_results
            orchestration_result['orchestration_metrics'] = self._calculate_orchestration_metrics(task_results)
            
            return orchestration_result
            
        except Exception as e:
            logger.error(f"Orchestration execution error: {e}")
            orchestration_result['error'] = str(e)
            return orchestration_result

    async def _initialize_specialist_coordination(self, specialist_assignments: Dict[str, dict]):
        """Initialize coordination with domain specialists."""
        unique_specialists = set(
            assignment['specialist'] 
            for assignment in specialist_assignments.values()
        )
        
        for specialist in unique_specialists:
            # Simulate specialist initialization (in real implementation, would use A2A protocol)
            self.active_agents[specialist] = {
                'status': 'initialized',
                'capabilities': self.domain_specialists.get(specialist, 'General specialist'),
                'tasks_assigned': [],
                'performance_metrics': {'tasks_completed': 0, 'avg_duration': 0}
            }
            
            logger.info(f"Initialized coordination with {specialist} specialist")

    async def _coordinate_sequential_execution(self, tasks: List[dict], sessionId: str) -> List[dict]:
        """Coordinate sequential task execution."""
        results = []
        
        for task in tasks:
            task_result = await self._coordinate_single_task(task, sessionId)
            results.append(task_result)
            
            # Update coordination history
            self.coordination_history.append({
                'task_id': task.get('id'),
                'status': task_result.get('status'),
                'timestamp': datetime.now().isoformat()
            })
        
        return results

    async def _coordinate_parallel_execution(self, tasks: List[dict], sessionId: str) -> List[dict]:
        """Coordinate parallel task execution."""
        # Create coroutines for parallel execution
        task_coroutines = [
            self._coordinate_single_task(task, sessionId) 
            for task in tasks
        ]
        
        # Execute in parallel
        results = await asyncio.gather(*task_coroutines, return_exceptions=True)
        
        # Process results and handle exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    'task_id': tasks[i].get('id', f'task_{i}'),
                    'status': 'error',
                    'error': str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results

    async def _coordinate_hybrid_execution(self, tasks: List[dict], sessionId: str) -> List[dict]:
        """Coordinate hybrid execution (parallel batches in sequence)."""
        # Group tasks by dependencies
        independent_tasks = [t for t in tasks if not t.get('dependencies', [])]
        dependent_tasks = [t for t in tasks if t.get('dependencies', [])]
        
        results = []
        
        # Execute independent tasks in parallel
        if independent_tasks:
            parallel_results = await self._coordinate_parallel_execution(independent_tasks, sessionId)
            results.extend(parallel_results)
        
        # Execute dependent tasks sequentially
        if dependent_tasks:
            sequential_results = await self._coordinate_sequential_execution(dependent_tasks, sessionId)
            results.extend(sequential_results)
        
        return results

    async def _coordinate_single_task(self, task: dict, sessionId: str) -> dict:
        """Coordinate execution of a single task via specialist."""
        try:
            task_id = task.get('id', 'unknown')
            task_description = task.get('description', '')
            
            # Simulate task coordination (in real implementation, would use A2A protocol)
            await asyncio.sleep(0.1)  # Simulate coordination time
            
            return {
                'task_id': task_id,
                'status': 'completed',
                'result': f'Coordinated completion: {task_description[:50]}...',
                'specialist_used': task.get('agent_type', 'generalist'),
                'coordination_time': 0.1
            }
            
        except Exception as e:
            return {
                'task_id': task.get('id', 'unknown'),
                'status': 'error',
                'error': str(e),
                'coordination_time': 0
            }

    async def _stream_orchestration(self, execution_plan: dict, sessionId: str):
        """Stream orchestration execution progress."""
        tasks = execution_plan.get('tasks', [])
        
        for i, task in enumerate(tasks, 1):
            yield {
                'response_type': 'text',
                'is_task_complete': False,
                'require_user_input': False,
                'content': f'âš¡ Coordinating task {i}/{len(tasks)}: {task.get("description", "")[:60]}...',
                'stage': 'task_coordination',
                'progress': i / len(tasks)
            }
            
            # Simulate coordination
            await asyncio.sleep(0.2)
            
            yield {
                'response_type': 'text',
                'is_task_complete': False,
                'require_user_input': False,
                'content': f'âœ… Task {i} coordinated: {task.get("description", "")[:40]}...',
                'stage': 'task_completed',
                'progress': i / len(tasks)
            }

    def _calculate_orchestration_metrics(self, task_results: List[dict]) -> dict:
        """Calculate orchestration performance metrics."""
        total_tasks = len(task_results)
        completed_tasks = len([r for r in task_results if r.get('status') == 'completed'])
        
        return {
            'total_tasks': total_tasks,
            'completed_tasks': completed_tasks,
            'success_rate': completed_tasks / total_tasks if total_tasks > 0 else 0,
            'coordination_efficiency': round(completed_tasks / max(total_tasks, 1), 2),
            'specialists_utilized': len(self.active_agents)
        }

    async def _synthesize_results(self, orchestration_result: dict, execution_plan: dict, original_query: str) -> dict:
        """Synthesize final orchestration results."""
        return {
            'status': 'completed',
            'domain': self.domain_name,
            'orchestrator_type': 'master_template_refactored',
            'original_query': original_query,
            'planning_summary': {
                'planner_used': 'EnhancedGenericPlannerAgent',
                'planning_mode': self.planner.planning_mode,
                'tasks_planned': len(execution_plan.get('tasks', [])),
                'coordination_strategy': execution_plan.get('coordination_strategy'),
                'risk_assessment': execution_plan.get('risk_analysis', {}).get('risk_level', 'unknown')
            },
            'orchestration_summary': orchestration_result.get('orchestration_metrics', {}),
            'execution_results': orchestration_result,
            'enhanced_capabilities': [
                'Strategic planning via Enhanced Planner Agent',
                'Risk assessment and mitigation analysis',
                'Resource estimation and optimization',
                'Timeline analysis and coordination',
                'Quality validation and scoring',
                'Template-based planning support'
            ],
            'timestamp': datetime.now().isoformat()
        }

    def _format_orchestrator_response(self, result: dict) -> str:
        """Format orchestrator response for consistency."""
        return json.dumps(result, indent=2)

    # ============================================================================
    # PHASE 1: Dynamic Workflow Graph Capabilities
    # ============================================================================
    
    def _ensure_dynamic_workflow(self, session_id: str) -> DynamicWorkflowGraph:
        """Ensure dynamic workflow exists for session."""
        if not self.enable_dynamic_workflow:
            logger.debug("Dynamic workflow disabled, using legacy workflow")
            return None
            
        # Create new workflow if session changed or no workflow exists
        if (self.current_session_id != session_id or 
            self.dynamic_workflow is None):
            
            # Clean up previous session workflows if session changed
            if (self.current_session_id and 
                self.current_session_id != session_id):
                self.clear_session_state()
            
            self.current_session_id = session_id
            self.dynamic_workflow = workflow_manager.create_workflow(session_id)
            logger.info(f"Created dynamic workflow {self.dynamic_workflow.workflow_id} for session {session_id}")
        
        return self.dynamic_workflow
    
    # ============================================================================
    # PHASE 2: Context & History Tracking Methods
    # ============================================================================
    
    def _initialize_session_context(self, session_id: str, query: str):
        """Initialize or update session context with query information."""
        if session_id not in self.session_contexts:
            self.session_contexts[session_id] = {
                'session_start': datetime.now().isoformat(),
                'domain': self.domain_name,
                'total_queries': 0,
                'query_types': {},
                'specialists_used': set(),
                'performance_summary': {},
                'context_version': 1
            }
        
        # Update session context
        context = self.session_contexts[session_id]
        context['total_queries'] += 1
        context['last_query'] = query
        context['last_activity'] = datetime.now().isoformat()
        
        # Track query patterns
        query_type = self._classify_query_type(query)
        context['query_types'][query_type] = context['query_types'].get(query_type, 0) + 1
        
        # Update global query patterns
        self.query_patterns[query_type] = self.query_patterns.get(query_type, 0) + 1
        
        logger.debug(f"Initialized context for session {session_id}, query type: {query_type}")
    
    def _classify_query_type(self, query: str) -> str:
        """Classify query into categories for pattern tracking."""
        query_lower = query.lower()
        
        # Domain-specific classification
        if any(word in query_lower for word in ['analyze', 'analysis', 'report', 'summary']):
            return 'analysis'
        elif any(word in query_lower for word in ['create', 'build', 'generate', 'develop']):
            return 'creation'
        elif any(word in query_lower for word in ['plan', 'strategy', 'roadmap', 'organize']):
            return 'planning'
        elif any(word in query_lower for word in ['optimize', 'improve', 'enhance', 'better']):
            return 'optimization'
        elif any(word in query_lower for word in ['fix', 'solve', 'resolve', 'debug']):
            return 'problem_solving'
        elif any(word in query_lower for word in ['research', 'find', 'search', 'discover']):
            return 'research'
        else:
            return 'general'
    
    def _record_execution_history(self, session_id: str, execution_record: Dict[str, Any]):
        """Record detailed execution history for analysis and learning."""
        if session_id not in self.execution_history:
            self.execution_history[session_id] = []
        
        # Enhance record with context
        enhanced_record = {
            **execution_record,
            'record_id': str(uuid.uuid4()),
            'domain': self.domain_name,
            'orchestrator_version': 'v2.0_refactored',
            'planner_mode': self.planner.planning_mode,
            'context_snapshot': self._get_context_snapshot(session_id)
        }
        
        # Add performance metrics
        plan_tasks = execution_record.get('execution_plan', {}).get('tasks', [])
        enhanced_record['performance_metrics'] = {
            'tasks_planned': len(plan_tasks),
            'execution_duration': execution_record.get('execution_duration', 0),
            'success_rate': self._calculate_success_rate(execution_record.get('orchestration_result', {})),
            'complexity_score': self._assess_execution_complexity(plan_tasks)
        }
        
        self.execution_history[session_id].append(enhanced_record)
        
        # Update session performance metrics
        self._update_session_performance(session_id, enhanced_record)
        
        # Track context evolution
        self._track_context_evolution(session_id, enhanced_record)
        
        logger.info(f"Recorded execution history for session {session_id}: {enhanced_record['record_id']}")
    
    def _get_context_snapshot(self, session_id: str) -> Dict[str, Any]:
        """Get current context snapshot for historical record."""
        return {
            'session_context': self.session_contexts.get(session_id, {}),
            'active_agents_count': len(self.active_agents),
            'workflow_state': self.dynamic_workflow.state.value if self.dynamic_workflow else 'none',
            'domain_context_keys': list(self.domain_context.keys()),
            'timestamp': datetime.now().isoformat()
        }
    
    def _calculate_success_rate(self, orchestration_result: Dict[str, Any]) -> float:
        """Calculate success rate from orchestration results."""
        metrics = orchestration_result.get('orchestration_metrics', {})
        total_tasks = metrics.get('total_tasks', 1)
        completed_tasks = metrics.get('completed_tasks', 0)
        
        return completed_tasks / total_tasks if total_tasks > 0 else 0.0
    
    def _assess_execution_complexity(self, tasks: List[Dict[str, Any]]) -> float:
        """Assess execution complexity based on task characteristics."""
        if not tasks:
            return 0.0
        
        complexity_factors = {
            'task_count': len(tasks),
            'parallel_tasks': len([t for t in tasks if not t.get('dependencies', [])]),
            'dependent_tasks': len([t for t in tasks if t.get('dependencies', [])]),
            'specialist_diversity': len(set(t.get('agent_type', 'generic') for t in tasks))
        }
        
        # Weighted complexity score
        score = (
            complexity_factors['task_count'] * 0.3 +
            complexity_factors['parallel_tasks'] * 0.2 +
            complexity_factors['dependent_tasks'] * 0.3 +
            complexity_factors['specialist_diversity'] * 0.2
        )
        
        return min(score / 10.0, 1.0)  # Normalize to 0-1 scale
    
    def get_session_context(self, session_id: str) -> Dict[str, Any]:
        """Get complete session context for analysis or debugging."""
        return {
            'session_context': self.session_contexts.get(session_id, {}),
            'execution_history': self.execution_history.get(session_id, []),
            'performance_metrics': self.performance_metrics.get(session_id, {}),
            'context_evolution': [
                entry for entry in self.context_evolution 
                if entry['session_id'] == session_id
            ][-10:]  # Last 10 evolution entries
        }
    
    def get_domain_insights(self) -> Dict[str, Any]:
        """Get domain-wide insights from accumulated context and history."""
        total_sessions = len(self.session_contexts)
        total_executions = sum(
            len(history) for history in self.execution_history.values()
        )
        
        # Calculate domain-wide averages
        all_performance = list(self.performance_metrics.values())
        avg_success_rate = sum(p['avg_success_rate'] for p in all_performance) / len(all_performance) if all_performance else 0
        avg_complexity = sum(p['avg_complexity'] for p in all_performance) / len(all_performance) if all_performance else 0
        
        return {
            'domain': self.domain_name,
            'total_sessions': total_sessions,
            'total_executions': total_executions,
            'query_patterns': self.query_patterns,
            'avg_success_rate': avg_success_rate,
            'avg_complexity': avg_complexity,
            'most_common_query_type': max(self.query_patterns.items(), key=lambda x: x[1])[0] if self.query_patterns else 'none',
            'performance_insights': self._generate_performance_insights()
        }
    
    def _generate_performance_insights(self) -> List[str]:
        """Generate actionable performance insights."""
        insights = []
        
        if not self.performance_metrics:
            return ['Insufficient data for performance analysis']
        
        # Analyze success rates
        success_rates = [m['avg_success_rate'] for m in self.performance_metrics.values()]
        avg_success = sum(success_rates) / len(success_rates)
        
        if avg_success < 0.8:
            insights.append('Success rate below optimal threshold - consider task decomposition review')
        elif avg_success > 0.95:
            insights.append('Excellent success rate - current strategies are highly effective')
        
        # Analyze complexity trends
        complexities = [m['avg_complexity'] for m in self.performance_metrics.values()]
        avg_complexity = sum(complexities) / len(complexities)
        
        if avg_complexity > 0.7:
            insights.append('High complexity tasks detected - consider specialist diversification')
        
        # Analyze query patterns
        if self.query_patterns:
            most_common = max(self.query_patterns.items(), key=lambda x: x[1])
            insights.append(f'Most common query pattern: {most_common[0]} ({most_common[1]} occurrences)')
        
        return insights
    
    def _update_session_performance(self, session_id: str, execution_record: Dict[str, Any]):
        """Update session-level performance metrics."""
        if session_id not in self.performance_metrics:
            self.performance_metrics[session_id] = {
                'total_executions': 0,
                'avg_duration': 0.0,
                'avg_success_rate': 0.0,
                'avg_complexity': 0.0,
                'total_tasks_executed': 0,
                'performance_trend': []
            }
        
        metrics = self.performance_metrics[session_id]
        perf = execution_record['performance_metrics']
        
        # Update aggregated metrics
        metrics['total_executions'] += 1
        n = metrics['total_executions']
        
        # Running averages
        metrics['avg_duration'] = ((metrics['avg_duration'] * (n-1)) + perf['execution_duration']) / n
        metrics['avg_success_rate'] = ((metrics['avg_success_rate'] * (n-1)) + perf['success_rate']) / n
        metrics['avg_complexity'] = ((metrics['avg_complexity'] * (n-1)) + perf['complexity_score']) / n
        metrics['total_tasks_executed'] += perf['tasks_planned']
        
        # Track performance trend
        metrics['performance_trend'].append({
            'timestamp': execution_record['timestamp'],
            'duration': perf['execution_duration'],
            'success_rate': perf['success_rate'],
            'complexity': perf['complexity_score']
        })
        
        # Keep only last 50 trend points to avoid memory bloat
        if len(metrics['performance_trend']) > 50:
            metrics['performance_trend'] = metrics['performance_trend'][-50:]
    
    def _track_context_evolution(self, session_id: str, execution_record: Dict[str, Any]):
        """Track how context evolves over time."""
        evolution_entry = {
            'session_id': session_id,
            'timestamp': execution_record['timestamp'],
            'query_type': self._classify_query_type(execution_record['query']),
            'complexity_change': self._detect_complexity_change(session_id, execution_record),
            'new_patterns': self._detect_new_patterns(execution_record),
            'context_version': self.session_contexts[session_id]['context_version']
        }
        
        self.context_evolution.append(evolution_entry)
        
        # Keep only last 100 evolution entries
        if len(self.context_evolution) > 100:
            self.context_evolution = self.context_evolution[-100:]
    
    def _detect_complexity_change(self, session_id: str, execution_record: Dict[str, Any]) -> str:
        """Detect if execution complexity is trending up, down, or stable."""
        if session_id not in self.performance_metrics:
            return 'initial'
        
        current_complexity = execution_record['performance_metrics']['complexity_score']
        avg_complexity = self.performance_metrics[session_id]['avg_complexity']
        
        if current_complexity > avg_complexity * 1.2:
            return 'increasing'
        elif current_complexity < avg_complexity * 0.8:
            return 'decreasing'
        else:
            return 'stable'
    
    def _detect_new_patterns(self, execution_record: Dict[str, Any]) -> List[str]:
        """Detect new patterns in execution that might indicate context shift."""
        patterns = []
        
        # Check for new specialist types
        plan_tasks = execution_record.get('execution_plan', {}).get('tasks', [])
        specialist_types = set(t.get('agent_type', 'generic') for t in plan_tasks)
        
        for specialist in specialist_types:
            if specialist not in self.domain_specialists:
                patterns.append(f'new_specialist:{specialist}')
        
        # Check for coordination strategy changes
        coord_strategy = execution_record.get('execution_plan', {}).get('coordination_strategy', 'sequential')
        if coord_strategy != 'sequential':  # Default strategy
            patterns.append(f'coordination:{coord_strategy}')
        
        # Check for execution duration anomalies
        duration = execution_record.get('execution_duration', 0)
        if duration > 30:  # Longer than 30 seconds
            patterns.append('long_execution')
        
        return patterns
    
    def add_graph_node(
        self,
        task_id: str,
        context_id: str, 
        query: str,
        node_id: Optional[str] = None,
        node_key: Optional[str] = None,
        node_label: Optional[str] = None
    ) -> Optional[WorkflowNode]:
        """
        Add a node to the dynamic workflow graph.
        
        Args:
            task_id: Task identifier
            context_id: Context identifier (session ID)
            query: Task query/description
            node_id: Parent node ID to connect to (optional)
            node_key: Node type key (optional)
            node_label: Human-readable label (optional)
            
        Returns:
            Created WorkflowNode or None if dynamic workflow disabled
        """
        workflow = self._ensure_dynamic_workflow(context_id)
        if not workflow:
            return None
            
        # Create new workflow node
        node = WorkflowNode(
            task=query,
            node_key=node_key,
            node_label=node_label or f"{self.domain_name} Task"
        )
        
        # Add metadata
        node.set_attributes({
            'task_id': task_id,
            'context_id': context_id,
            'domain': self.domain_name,
            'orchestrator_type': 'master_template_refactored'
        })
        
        # Add to workflow
        workflow.add_node(node)
        
        # Add edge if parent specified
        if node_id and node_id in workflow.nodes:
            workflow.add_edge(node_id, node.id)
        
        logger.info(f"Added workflow node {node.id} ({node_key}) to {workflow.workflow_id}")
        return node
    
    def set_node_attributes(
        self, 
        node_id: str, 
        task_id: Optional[str] = None,
        context_id: Optional[str] = None, 
        query: Optional[str] = None,
        **additional_attrs
    ):
        """
        Set attributes on a workflow node.
        
        Args:
            node_id: Target node ID
            task_id: Task ID to set (optional)
            context_id: Context ID to set (optional)
            query: Query to update (optional)
            **additional_attrs: Additional attributes to set
        """
        if not self.dynamic_workflow:
            logger.debug("No dynamic workflow available for setting node attributes")
            return
            
        attributes = {}
        if task_id:
            attributes['task_id'] = task_id
        if context_id:
            attributes['context_id'] = context_id
        if query:
            # Update node task as well
            node = self.dynamic_workflow.get_node(node_id)
            if node:
                node.task = query
        
        attributes.update(additional_attrs)
        
        if attributes:
            self.dynamic_workflow.set_node_attributes(node_id, attributes)
    
    def get_workflow_stats(self) -> Dict[str, Any]:
        """Get current workflow statistics."""
        if not self.dynamic_workflow:
            return {'dynamic_workflow_enabled': False}
            
        stats = self.dynamic_workflow.get_workflow_stats()
        stats['dynamic_workflow_enabled'] = True
        stats['domain'] = self.domain_name
        return stats
    
    def clear_session_state(self):
        """Clear state for session transitions."""
        if self.current_session_id:
            workflow_manager.cleanup_session(self.current_session_id)
            logger.info(f"Cleared session state for {self.current_session_id}")
        
        # Reset instance state
        self.dynamic_workflow = None
        self.current_session_id = None
        self.active_agents.clear()
        self.execution_context.clear()
        self.coordination_history.clear()
        self.workflow_graph = None
    
    def pause_workflow(self, paused_node_id: Optional[str] = None):
        """Pause the current workflow."""
        if self.dynamic_workflow:
            self.dynamic_workflow.pause_workflow(paused_node_id)
            logger.info(f"Paused workflow at node {paused_node_id}")
    
    def resume_workflow(self):
        """Resume the current workflow."""
        if self.dynamic_workflow:
            self.dynamic_workflow.resume_workflow()
            logger.info("Resumed workflow execution")
    
    def get_paused_node_id(self) -> Optional[str]:
        """Get the ID of the paused node."""
        if self.dynamic_workflow:
            return self.dynamic_workflow.paused_node_id
        return None
    
    def is_workflow_paused(self) -> bool:
        """Check if workflow is currently paused."""
        if self.dynamic_workflow:
            return self.dynamic_workflow.state == WorkflowState.PAUSED
        return False
    
    def get_executable_nodes(self) -> List[WorkflowNode]:
        """Get nodes that can be executed now."""
        if self.dynamic_workflow:
            return self.dynamic_workflow.get_executable_nodes()
        return []
    
    def _build_workflow_from_plan(self, execution_plan: Dict[str, Any], session_id: str) -> Optional[str]:
        """
        Build dynamic workflow graph from execution plan.
        
        Args:
            execution_plan: Plan from Enhanced Planner Agent
            session_id: Session identifier
            
        Returns:
            Starting node ID or None if dynamic workflow disabled
        """
        workflow = self._ensure_dynamic_workflow(session_id)
        if not workflow:
            return None
            
        tasks = execution_plan.get('tasks', [])
        if not tasks:
            logger.warning("No tasks in execution plan for workflow building")
            return None
        
        # Start workflow execution
        workflow.start_workflow()
        
        # Create planner node as root
        planner_node = self.add_graph_node(
            task_id="planner",
            context_id=session_id,
            query="Strategic planning and task decomposition",
            node_key="planner",
            node_label="Strategic Planner"
        )
        
        if not planner_node:
            return None
            
        # Mark planner as completed since we have the plan
        planner_node.complete_execution(execution_plan)
        
        # Add task nodes based on coordination strategy
        coordination_strategy = execution_plan.get('coordination_strategy', 'sequential')
        current_node_id = planner_node.id
        
        for idx, task in enumerate(tasks):
            task_node = self.add_graph_node(
                task_id=task.get('id', f"task_{idx}"),
                context_id=session_id,
                query=task.get('description', f"Execute task {idx + 1}"),
                node_id=current_node_id if coordination_strategy == 'sequential' else planner_node.id,
                node_key=task.get('specialist_type', 'generic'),
                node_label=task.get('title', f"Task {idx + 1}")
            )
            
            if task_node:
                # Add task-specific metadata
                task_node.set_attributes({
                    'task_data': task,
                    'specialist_type': task.get('specialist_type'),
                    'priority': task.get('priority', 'medium'),
                    'estimated_duration': task.get('estimated_duration')
                })
                
                # Update current node for sequential chaining
                if coordination_strategy == 'sequential':
                    current_node_id = task_node.id
        
        logger.info(f"Built dynamic workflow with {len(tasks)} task nodes, strategy: {coordination_strategy}")
        return planner_node.id

    def get_orchestrator_capabilities(self) -> dict:
        """Get comprehensive orchestrator capabilities."""
        return {
            'orchestrator_type': 'master_template_refactored',
            'domain': self.domain_name,
            'architecture': 'Enhanced Planner Agent integration',
            'planning_capabilities': {
                'planner_type': 'EnhancedGenericPlannerAgent',
                'planning_modes': ['simple', 'sophisticated'],
                'current_mode': self.planner.planning_mode,
                'advanced_features': [
                    'Task decomposition and analysis',
                    'Risk assessment and mitigation',
                    'Resource estimation and budgeting',
                    'Plan validation and feasibility checks',
                    'Template library and application',
                    'Plan versioning and iteration',
                    'Cost and timeline estimation',
                    'Quality scoring and validation'
                ]
            },
            'orchestration_capabilities': [
                'Domain specialist coordination',
                'Sequential execution management',
                'Parallel execution coordination',
                'Hybrid execution strategies',
                'A2A protocol integration',
                'Performance monitoring and metrics',
                'Health checking and diagnostics'
            ],
            'dynamic_workflow_capabilities': [
                'Dynamic graph building with nodes and edges',
                'Task metadata and execution tracking', 
                'State management (RUNNING, PAUSED, COMPLETED)',
                'Node attribute management for orchestration',
                'Workflow pause and resume functionality',
                'Session-based workflow isolation',
                'Dependency resolution and execution ordering',
                'Integration with Enhanced Planner Agent'
            ] if self.enable_dynamic_workflow else [],
            'context_history_capabilities': [
                'Session-based context tracking and management',
                'Query pattern recognition and classification',
                'Execution history recording with rich metadata',
                'Performance metrics tracking and analysis',
                'Context evolution monitoring and trend analysis',
                'Domain-wide insights and performance optimization',
                'Context snapshots for historical analysis',
                'Complexity change detection and alerting',
                'Pattern-based intelligence for future planning'
            ],
            'backward_compatibility': 'Full API compatibility with original MasterOrchestratorTemplate',
            'enhanced_features': 'All capabilities enhanced via EnhancedGenericPlannerAgent integration',
            'phase_completion_status': {
                'phase_1_dynamic_workflow': True,
                'phase_2_context_history': True,
                'phase_3_state_management': False,
                'phase_4_artifact_management': False
            }
        }